{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a675dfaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/gmongaras/miniconda3/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-04-10 08:00:41.328607: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-04-10 08:00:42.127735: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:17<00:00,  4.33s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from copy import deepcopy\n",
    "from datasets import load_dataset\n",
    "os.environ[\"HF_Home\"] = \"cache\"\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "with open(\".env\") as f:\n",
    "    token = f.read().strip()\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "quantization_config = BitsAndBytesConfig(load_in_8bit=False)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\", token=token, use_fast=True, cache_dir=\"cache\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\", token=token, cache_dir=\"cache\", torch_dtype=torch.bfloat16, device_map=\"cuda:0\", quantization_config=quantization_config)\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3-8B\", load_in_4bit=True, token=token, cache_dir=\"cache\", device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e47f8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = torch.load(\"models/policy_1200.pt\", weights_only=False).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "765a2a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "group_batch_size = 4\n",
    "num_steps = 1000\n",
    "max_length = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d80229e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.model_max_length = max_length\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea5002a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Answer: 1B\n",
      "\n",
      "\n",
      "\n",
      "Full output-------------------\n",
      "system\n",
      "\n",
      "You will be given a question. First reason about the question and answer. Then answer the question.\n",
      "The final solution must be in json format:\n",
      "{\n",
      "    \"solution\": \"put your solution here\"\n",
      "}\n",
      "Your solution should only include the final answer in the format above.user\n",
      "\n",
      "Question: Maya packed a gift box for her cousin. She first placed the empty box on a scale and added enough cookies to make the box weigh 3 pounds. Then, she added enough chocolate bars to double the weight. After that, she added another 3 pounds of cookies. Finally, she added enough candy canes to triple the weight of the box at that point. What was the final weight of the box, in pounds? Translate the answer to binary. then translate it to hex from binary.assistant\n",
      "\n",
      "Step 1: Analyze the problem\n",
      "\n",
      "Maya starts with a 0-pound box and adds weights incrementally.\n",
      "\n",
      "Step 2: Determine the weight after each addition\n",
      "\n",
      "1. Initial weight = 0\n",
      "2. First addition: Cookies add 3 pounds to make the box weigh 3 pounds.\n",
      "3. Second addition: Chocolate bars double the weight to 6 pounds.\n",
      "4. Third addition: Another 3 pounds of cookies make the box weigh 9 pounds.\n",
      "5. Fourth addition: Candy canes triple the weight to 27 pounds.\n",
      "\n",
      "Step 3: Convert the final weight to binary\n",
      "Binary of 27 = 11011\n",
      "\n",
      "Step 4: Convert binary to hex\n",
      "Hex of 11011 = 1B\n",
      "\n",
      "{\n",
      "    \"solution\": \"1B\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Generate a  \n",
    "question = \"Maya packed a gift box for her cousin. She first placed the empty box on a scale and added enough cookies to make the box weigh 3 pounds. Then, she added enough chocolate bars to double the weight. After that, she added another 3 pounds of cookies. Finally, she added enough candy canes to triple the weight of the box at that point. What was the final weight of the box, in pounds? Translate the answer to binary. then translate it to hex from binary.\" \n",
    "prompt = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "You will be given a question. First reason about the question and answer. Then answer the question.\n",
    "The final solution must be in json format:\n",
    "{\n",
    "    \"solution\": \"put your solution here\"\n",
    "}\n",
    "Your solution should only include the final answer in the format above.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "\"\"\" + f\"Question: {question}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "max_length = 512\n",
    "temperature = 0.7\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\", padding_side=\"left\")\n",
    "# Generate text\n",
    "outputs = policy.generate(input_ids=inputs[\"input_ids\"].cuda(), attention_mask=inputs[\"attention_mask\"].cuda(), max_length=max_length, do_sample=True, temperature=temperature)\n",
    "out = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "search_string = \"\"\"\n",
    "{\\s*\"solution\": .*\\s*}\n",
    "\"\"\".strip()\n",
    "ans = json.loads(re.findall(search_string, out)[-1])[\"solution\"]\n",
    "print(f\"\\n\\n\\nAnswer: {ans}\\n\\n\\n\")\n",
    "print(f\"Full output-------------------\\n{out}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405e0365",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
