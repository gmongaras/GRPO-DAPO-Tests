{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a675dfaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/gmongaras/miniconda3/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-04-08 08:13:15.773091: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-04-08 08:13:16.567646: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:17<00:00,  4.25s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from copy import deepcopy\n",
    "from datasets import load_dataset\n",
    "os.environ[\"HF_Home\"] = \"cache\"\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "with open(\".env\") as f:\n",
    "    token = f.read().strip()\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "quantization_config = BitsAndBytesConfig(load_in_8bit=False)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\", token=token, use_fast=True, cache_dir=\"cache\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\", token=token, cache_dir=\"cache\", torch_dtype=torch.bfloat16, device_map=\"cuda:0\", quantization_config=quantization_config)\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3-8B\", load_in_4bit=True, token=token, cache_dir=\"cache\", device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e47f8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = torch.load(\"models/policy_780.pt\", weights_only=False).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "765a2a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "group_batch_size = 4\n",
    "num_steps = 1000\n",
    "max_length = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d80229e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.model_max_length = max_length\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea5002a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system\n",
      "\n",
      "You will be given a question. First reason about the question and answer. Then answer the question.\n",
      "The final solution must be in json format:\n",
      "{\n",
      "    \"solution\": \"put your solution here\"\n",
      "}\n",
      "Your solution should only include the final answer in the format above.user\n",
      "\n",
      "Question: What is the largest 5 digit number between the 32nd and 50th digits of pi?assistant\n",
      "\n",
      "To solve this problem, we first need to know the approximate value of pi between the 32nd and 50th digits. \n",
      "\n",
      "Pi is approximately 3.1415926535897932384626433832795028841971693993751058209749445923078164062862089986280348253421170679...\n",
      "\n",
      "The 32nd digit is 5 and the 50th digit is 9. \n",
      "\n",
      "The largest 5 digit number between the 32nd and 50th digits of pi would be the largest 5 digit number that is less than the 50th digit of pi. Since the 50th digit of pi is 9, the largest 5 digit number less than 9 is 99999. \n",
      "\n",
      "{\n",
      "    \"solution\": 99999\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Generate a  \n",
    "question = \"Maya packed a gift box for her cousin. She first placed the empty box on a scale and added enough cookies to make the box weigh 3 pounds. Then, she added enough chocolate bars to double the weight. After that, she added another 3 pounds of cookies. Finally, she added enough candy canes to triple the weight of the box at that point. What was the final weight of the box, in pounds?\" \n",
    "prompt = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "You will be given a question. First reason about the question and answer. Then answer the question.\n",
    "The final solution must be in json format:\n",
    "{\n",
    "    \"solution\": \"put your solution here\"\n",
    "}\n",
    "Your solution should only include the final answer in the format above.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "\"\"\" + f\"Question: {question}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "max_length = 512\n",
    "temperature = 0.7\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\", padding_side=\"left\")\n",
    "# Generate text\n",
    "outputs = policy.generate(input_ids=inputs[\"input_ids\"].cuda(), attention_mask=inputs[\"attention_mask\"].cuda(), max_length=max_length, do_sample=True, temperature=temperature)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
